\documentclass{article}

\usepackage[margin=1in]{geometry}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}

% bib
\usepackage[round]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}
%\usepackage{algorithmic}
\usepackage{algpseudocode}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{booktabs}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Latent Constituents via Self-Attention}

\begin{document}
\maketitle

\begin{comment}
\section{Introduction}

\section{Problem}
We would like to learn a generative model
over source sentences $\bx = \{x_0, x_1, \ldots\}$,
using a distribution over latent trees $\bz$.  
We are primarily interested in the posterior distribution over trees given a sentence, $p(\bz\mid\bx)$.
The performance of the generative model is secondary since we would like to analyze the
output of the unsupervised model for any linguistic regularities, and use the model as a 
testbed for linguistic hypotheses (of which I have none at the moment).

\citep{shen2018prpn}
\citet{yin18structvae}
\end{comment}

\section{PRPN \citep{shen2018prpn}}
The Parsing-Reading-Predict Network \citep{shen2018prpn} is an approach to unsupervised constituency tree induction.
%that uses a soft approximation to a latent variable to a degree of success.
PRPN achieves this by learning a scoring model that limits the range of self-attention
in a self-attentive language model, and then inducing tree structure using the scoring model.

\section{The Language Model}
In addition to modeling the tokens,
\citet{shen2018prpn} introduce a latent variable at every timestep $t$. 
Let $\bx$ be all tokens
and $\bl$ all latent variables with each $l_t\in\set{0,\ldots,t-1}$ a categorical RV over all previous indices.
They decompose the joint distribution 
\begin{equation}
p(\bx, \bl)
%= \prod_t p(x_{t+1},l_t\mid\bx_{\le t},\bl_{<t})
= \prod_t \underbrace{p(x_{t+1}\mid l_t,\bx_{\le t},\bl_{<t})}_{\textrm{Self-attentive LM}}
\underbrace{p(l_t\mid\bx_{\le t})}_{\textrm{Attention limit}}.
\end{equation}

\subsection{$p(x_{t+1}\mid l_t,\bx_{\le t},\bl_{<t})$}
The first term takes the form of a self-attentive language model,
where the distribution over the next token is
$$p(x_{t+1}\mid l_t,\bx_{\le t},\bl_{<t}) = \textrm{softmax}(f(\bm{m}_{<t}, l_t))$$
where $f$ is either a linear projection or additional residual blocks,
composed with the output of an LSTMN (LSTM Memory-Network),
as well as the latent variable $l_t$.
The portion of this network responsible for directly predicting the next word is
referred to as the Predict Network, and the LSTMN is the Reading Network.
We will define $l_t$ more rigorously in the following section.
We can interpret $l_t$ as the leftmost limit of self-attention for the given timestep
such that the model cannot assign any attention mass to a memory at index $< l_t$.
The LSTMN attends over the previous $\bm{m}_{<t}$ to produce $m_t$,
the current hidden state.
The input to the LSTMN is the current token $x_t$.

The attention mechanism is defined similarly throughout their network.
The recurrent input to the current timestep is a convex combination of
all previous outputs of the current layer.
Let $\bm{m}_{<t}$ be the concatenated output of all previous timesteps,
then the recurrent input would be
\begin{equation}c = \sum_{i=0}^{t-1} s_i(l_t) m_i\end{equation}
and the output of the layer at the current timestep would be
$m_t = LSTM(c, x_t)$ where $s_i$ is the attention coefficient
for that index at timestep $t$ and $x_t$ is the input.

The attention coefficients are a renormalized weighted sum of
the dot-product attention we are used to.
Let $\tilde{\bm{s}_t}\in\Delta^{t-2}$, the $t-2$ simplex, be the output of a softmax over
the dot-product of memories $\bm{m}_{<t}$ and query $k_t$
which is a linear function of the input $x_t$.
We then define
\begin{equation}s_i(l_t) = \frac{g_i(l_t)\tilde{s}_i}{\sum_jg_j(l_t)\tilde{s}_j},\end{equation}
where $g_i(l_t) = \mathbf{1}(l_t \le i)$.
We have a set of these \textbf{per timestep}.
Note that although we define $p(l_t)$ in the following section,
we will end up using $\Es{l_t}{s_i(l_t)}$ as a soft approximation to the
latent variable model.
The LSTMN is referred to as the Reading Network.

[Work through Figure~\ref{fig:hard}]

\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 0 & 1 & 2 & 3 & 4\\
\hline
raw attn $\tilde{s}$ & 0.4 & 0.1 & 0.3 & 0.2 & \\
\hline
gates $g\mid l_4=2$         & 0   & 1   & 1   & 1   & - \\
\hline
attn $s(l_4=2)$     & 0 & 0.17 & 0.5 & 0.33 & -\\
\hline
\end{tabular}
\caption{An example computation of the attention gates with the LVM. Let $t=4$.}
\label{fig:hard}
\end{figure}
\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 0 & 1 & 2 & 3 & 4\\
\hline
raw attn $\tilde{s}$ & 0.4 & 0.1 & 0.3 & 0.2 & \\
\hline
scores $d$         & 0.7 & 0.6 & 0.3 & 0.4 & 0.5\\
\hline
$\alpha$    & - & 0.45 & 0.6 & 0.55 & -\\
\hline
$p(l_4)$    & 0.45*0.6*0.55=0.1485 & (1-0.45)*0.6*0.55=0.1815 & (1-0.6)*0.55=0.22 & 1-0.55=0.45 &\\
\hline
cdf $F_{l_4}$   & 0.1485 & 0.33 & 0.55 & 1 &\\
\hline
soft attn $s(\E{l_4})$ & 0.13 & 0.07 & 0.36 & 0.43 & -\\
\hline
hard attn $s(l_4=2)$     & 0 & 0.17 & 0.5 & 0.33 & -\\
\hline
\end{tabular}
\caption{An example computation of the attention gates.
Let $\alpha_i = \frac{d_t-d_i+1}{2}$ and $t=4$.}
\label{fig:soft}
\end{figure}

\subsection{$p(l_t\mid\bx_{\le t})$}
We define
\begin{equation}
p(l_t=k\mid\bx_{\le t}) =\left\{\begin{array}{l l}
1-\alpha_{t-1}, & k=t-1\\
\prod_{j=1}^{t-1}\alpha_j, & k=0\\
(1-\alpha_k)\prod_{j=k+1}^{t-1}\alpha_j, & \textrm{otherwise}
\end{array}\right.
\end{equation}
Where the $\alpha_k$ are drawn from an unspecified Beta distribution.
Thus $l_t\sim\Cat(\theta)$ where the parameters $\theta$ are determined
through a stick-breaking process. 
[Go over stick breaking process on board.]
%This ends up looking more like a generalized geometric distribution
%where probability of success at each time step is flexible and there is a 
%maximum number of trials.
This hints at an order-statistic interpretation which 
we will touch upon if there is time.

Note that this cannot be called a Dirichlet Process, as a $DP(\alpha, H_0)$ is defined by
$b_i\sim\Betad(1,\alpha),\pi_i=(1-b_i)\prod_{j=0}^{i-1}b_i,\bm{\pi}\sim GEM(\alpha)$
and with $\theta_i\sim H_0$ we have $G = \sum_i\pi_i\delta_{\theta_i}\sim DP(\alpha, H_0)$.
This would only make sense in this context if the base distribution $H_0$ were also Dirichlet,
but this would likely warrant further discussion.

Rather than proceed further in defining the generative model,
\citet{shen2018prpn} pivot and use $\Es{l_t}{s_i(l_t)}$ instead of $s_i(l_t)$.
We have
\begin{equation}
\Es{l_t}{g_i(l_t)} = \prod_{j=i+1}^{t-1}\alpha_j,
\end{equation}
and we then use
\begin{equation}
\Es{l_t}{s_i(l_t)} = \frac{\Es{l_t}{g_i(l_t)}\tilde{s}_i}{\sum_j\Es{l_t}{g_j(l_t)}\tilde{s}_j}
\end{equation}
as the attention coefficients instead of $s_i(l_t)$.
They then parameterize the $\alpha_k$ of timestep $t$ deterministically as follows:
\begin{equation}
\alpha_k = \frac{\textrm{hardtanh}(2(d_t-d_k)/\tau+1)+1}{2},
\end{equation}
where $\tau$ is some scaling parameter.
Any function whose range is $[0,1]$ should be fine for a stick-breaking definition.
The scores $d_k\in\mathbb{R}^+$ are obtained from a CNN over the tokens $\bx_{k-L:k}$,
where $L$ is the width of the convolution kernel (this is set to 5).
The scores $d_k$ are output by the Parsing Network.

[Work through Figure~\ref{fig:soft}]

\section{Training}
With the soft model that uses $\E{s_i(l_t)}$ training is very simple.
We simply maximize the likelihood of the data directly since the model has no latent variables.

\section{Extracting a Parse Tree}
Given a trained model, we can extract a binary constituency tree using the scores from the parsing network.
Perform the following procedure recursively:
\begin{algorithmic}
\Function{Split}{$x = \set{x_0,\ldots,x_T}$}
\If{$x = \emptyset$}
\State \Return $\emptyset$
\ElsIf{$|x| = 1$}
\State \Return $x$
\Else
\State Choose index $i\in\set{0,\ldots,T}$ 
\State \Return $(\Call{Split}{\bx_{<i}}, (x_i, \Call{Split}{\bx_{>i}}))$
\EndIf
\EndFunction
\end{algorithmic}
The definition then reduces to defining the index choice step.
Under an order-statistic model we could sample $i\sim\Cat(\textrm{softmax}(\bd))$
or use a greedy approximation by taking the argmax of that distribution at every step.

\section{Results}
\subsection{Language Modeling}

The language modeling results are decent, but it's unclear whether the
gating mechanism offers any improvement.
They report the results of an ablation study, but they do not seem to be replicable.
On github someone reported that removing the Parsing Network had very little effect on perplexity.
It is possible the perplexity improvements come 
mainly from embedding-tying and partly from self-attention.

SOTA is around 55 and \citet{shen2018prpn} are around 62.

\begin{table}[h!]                                                                 
\centering                                                                       
  \begin{tabular}{ c c }                                                         
    \toprule[2pt]                                                                
    Model & PPL \\                                                               
    \hline                                                                       
    RNN-LDA + KN-5 + cache \citep{mikolov2012context} &  92.0 \\                 
    LSTM \citep{zaremba2014recurrent} & 78.4 \\                                  
    Variational LSTM \citep{kim2016character} & 78.9 \\                          
    CharCNN \citep{kim2016character} & 78.9 \\                                   
    Pointer Sentinel-LSTM \citep{merity2016pointer} & 70.9 \\                    
    LSTM + continuous cache pointer \citep{grave2016improving} & 72.1 \\         
    Variational LSTM (tied) + augmented loss \citep{inan2016tying} & 68.5 \\     
    Variational RHN (tied) \citep{zilly2016recurrent} & 65.4 \\                  
    NAS Cell (tied)  \citep{zoph2016neural} & 62.4 \\                            
    4-layer skip connection LSTM (tied) \citep{melis2017state} & \textbf{58.3} \\
%     3-layer AWD-LSTM (tied) \citep{merity2017regularizing} & \textbf{57.3} \\  
    \hline                                                                       
    PRPN & 61.98 \\                                                              
    \toprule[2pt]                                                                
  \end{tabular}                                                                  
  \caption{PPL on the Penn Treebank test set}                                    
  \label{tab_ptb_word}                                                           
\end{table}                                                                      


\subsection{Parsing}
The parsing results, on the other hand, are quite good.
On WSJ10, the model beats the right-branching baseline but not the CCM model.
On the full WSJ, the model gets an averaged sentence F1 of 38.1,
which beats random, balanced, and right branching baselines
(the highest F1 reported is 21.3).
Why should we care about this?
There is no reason apriori that limiting self-attention should work at all
for learning to rank, as a fully left-branching structure would
allow the self-attention to function as normal. 

\begin{table}[h!]                                    
\centering
  \begin{tabular}{ c c }
    \toprule[2pt]
    Model & $\mathrm{UF}_1$ \\
    \hline
    LBRANCH &  28.7 \\
    RANDOM & 34.7 \\
    DEP-PCFG \citep{carroll1992two} & 48.2 \\
    RBRANCH & 61.7 \\
    CCM \citep{klein2002generative} & 71.9 \\
    DMV+CCM \citep{klein2005natural} & 77.6 \\
    UML-DOP \citep{bod2006all} & \textbf{82.9} \\
    \hline
    PRPN & 66.4 \\
    \hline                                          
    UPPER BOUND & 88.1 \\                           
    \toprule[2pt]                                   
  \end{tabular}                                     
  \caption{Parsing Performance on the WSJ10 dataset}
  \label{tb_parser}                                 
\end{table}                                         

\subsection{Alternative Probabilistic Interpretation}
Rather than defining latent variable $l_t$ which is a categorical over indices,
what if we instead define $l_t$ as a function of pairwise comparisons between $d_t$
and all preceeding $d_i$ (for a given timestep $t$)?
We would have
$$l_t = \max \set{i : \alpha_i = 1, 0\leq i<t},
\alpha_i \sim \Bern\left(\frac{d_i}{d_t+d_i}\right).$$
This yields
$p(\alpha_i=1)=\frac{d_i}{d_t+d_i}, \forall i\in\set{1,t-1}$.
Similarly to the stick-breaking procedure, $p(\alpha_i=0)$ is already defined as the remaining mass.
We then have
\begin{equation*}
p(l_t=k) =\left\{\begin{array}{l l}
p(\alpha_{t-1}), & k=t-1\\
\prod_{j=1}^{t-1}1-p(\alpha_j), & k=0\\
p(\alpha_k)\prod_{j=k+1}^{t-1}1-p(\alpha_j), & \textrm{otherwise}
\end{array}\right.
\end{equation*}
Again, we would have $l_t\sim\Cat$ parameterized as above.
However, we could use hard attention tricks to maximize the marginal likelihood of this model exactly
and the generative model is actually well-defined.
This is known as a Bradley-Terry pairwise order statistic model.
See Figure~\ref{fig:geom} for a worked example of some of this process.
Under this model, the implementation of $l_t$ can be interpreted as allowing the model to attend up through 
the nearest token to the left that has higher rank.
This has a tree interpretation, but it is unintuitive.
Once we learn a pairwise model, we can use the parameterization of the scores $d_i$
to define a model over trees.
The tree-based interpretation would then be:
\begin{itemize}
\item[(a)] either the token at $l_t$ is the leftmost sibling of the token at $t$
\item[(b)] or if the token at $t$ is a leftmost child, $l_t$ points to its parent's left sibling's leftmost child.
\end{itemize}

To define a distribution over trees using scores, we follow a generative story
similar to the split function.
In order to define a tree, we recursively partition $\bx$:
\begin{equation}
p(\bx;\bd) = p(\textrm{choose} = i; \bd)p(\bx_{<t};\bd_{<t})p(\bx_{>t};\bd_{>t}),
\end{equation}
where $p(\textrm{choose} = i;\bd)=\frac{d_i}{\sum_j d_j}$.

If we follow the split function exactly, we would instead define a distribution over rankings:
\begin{equation}
p(\bx;\bd) = p(\textrm{choose}= i; \bd)p(\bx\setminus\set{x_i}; \bd\setminus\set{d_i}),
\end{equation}
where the choice is parameterized in exactly the same way.
Under this model, the split procedure can be seen as a greedy approach to finding
the most likely tree under this distribution.

\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 0 & 1 & 2 & 3 & 4\\
\hline
scores $d$         & 0.7 & 0.6 & 0.3 & 0.4 & 0.5\\
\hline
$p(\alpha_i=1)$    & - & 6/11 & 3/8 & 4/9 & -\\
\hline
%$p(l_4)$ & (1-4/9)*(1-3/8)*(1-6/11) & (1-4/9)*(1-0.37)*0.55=0.19 & (1-0.45)*0.3/(0.3+0.5)= 0.21 & 0.4/(0.4+0.5)=0.45 &\\
$p(l_4=k)$ & 0.16 & 0.19 & (1-0.45)*3/8= 0.21 & 4/9=0.45 & -\\
\hline
\end{tabular}
\caption{An example computation of the attention gates.
with a pairwise ranking model and $t=4$.
It gives very similar probabilities when compared to the `stick-breaking' approach but is tractable.}
\label{fig:geom}
\end{figure}
\begin{comment}
\section{other stuff}
Let $\pi(i)$ denote the rank given to item $i$, and $\pi^{-1}(j)$ denote the 
item at rank $j$.
\begin{enumerate}
\item Plackett-Luce:
There is a hidden score $\theta_i$ assigned to every item $i$.
The $j$th ranking item is chosen from a categorical distribution 
parameterized by the output of a softmax over the remaining items' scores.
Here we have $\bz = \{\pi^{-1}(0),\pi^{-1}(1),\ldots,\pi^{-1}(T)\}$.
The probability of a given permutation is
$$
p(\bz)
= \prod_{i=0}^T\frac{\exp(\theta_{z_i})}{\sum_{k=i}^T \exp(\theta_{z_k})}
= \prod_{i=0}^T\frac{\exp(\theta_{\pi^{-1}(i)})}{\sum_{k=i}^T \exp(\theta_{\pi^{-1}(k)})}
$$
\item Pairwise Comparison Model (Bradley-Terry):
The same as PL, but samples pairwise comparisons from a bernoulli distribution
using the item's scores.
The probability that item $i$ is ranked higher than item $k$ is 
$\frac{\theta_{z_i}}{\theta_{z_i}+\theta_{z_k}}$.
Similary, we have the probability of a permutation is
$$
p(\bz) 
= \prod_{i=0}^T\prod_{k=i+1}^T\frac{\theta_{z_i}}{\theta_{z_i}+\theta_{z_k}}
$$
\end{enumerate}
One could use the Bradley-Terry (pairwise)
model of ranking for training a language model,
but then switch to the Plackett-Luce model at test time for outputting parse trees.
This would allow us to learn a scoring model which can then be shared between the 
pairwise and global ranking models.

The paper is inspired by the following hypothesis:
given a binary tree, a token at index $t$ only requires information up to index $l_t$ that satisfies
either of the following conditions:
\begin{itemize}
\item[(a)] the token at $l_t$ is the leftmost sibling of the token at $t$
\item[(b)] or, if the token at $t$ is a leftmost child, $l_t$ points to its parent's left sibling's leftmost child.
%\end{itemize}
%Although the hypothesis itself is not tested in the implementation and serves only as inspiration,
%the model does see empirical success in the task of language modeling.
The model is realized through the following insight: given a ranking of tokens $\bx = \{x_0,\ldots,x_T\}$,
recursively splitting $\bx$ using the following procedure induces a binary tree where the first token to the left
of token $x_t$ that has higher rank, denoted $x_{l_t}$, also satisfies condition (a) or (b):
given token $i$ with the next highest rank, create a subtree $(x_{<i}, (x_i, x_{>i}))$ and
recursively perform this procedure until only terminal nodes remain.

\section{Comparison to CCM \citep{klein-2002-ccm}}
\citep{klein-2002-ccm2}
\citep{golland-2012-ccm,huang-2012-ccm}
\section{Comparison to StructVae \citep{yin18structvae}}
Hm, if we generated context given span, would that encourage non-constituents?
General question about segmental models.

\section{Related Work}

\section{Alternative Latent Ranking Model}
\subsection{Generative Model}
PRPN \citep{shen2018prpn} choose to view $l_t$ as a latent variable.
There are at least a few other choices:
\begin{itemize}
\item Model the scores $d_t \sim \mathcal{N}(\mu_t,\sigma_t)$
or $d_t \sim \textrm{Gamma}$
and use the Plackett-Luce ranking distribution.
\item Model the comparisons $p(d_t < d_i)$ as order statistics of Gammas.
\item Model the permutation matrix $Z \sim \mathcal{B}_n$.
\item Model $l_t\sim \textrm{Cat}$.
\end{itemize}
Parameterize with $d_t\sim\mathcal{N}(\mu_t, \sigma_t)$.
Reparameterize comparisons with gumbel softmax?
Leave all self attentions as is?
\begin{itemize}
\item $p(z)$
\item $p(x|z)$
\end{itemize}

\section{Training and Inference}

for next time, convince Sasha of why this is interesting.
Also go over stick breaking process.
\end{comment}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

