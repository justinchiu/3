\documentclass{article}

\usepackage[margin=1in]{geometry}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}

% bib
\usepackage[round]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{booktabs}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Latent Constituents via Self-Attention}

\begin{document}
\maketitle

\section{Introduction}


\section{Problem}
We would like to learn a generative model
over source sentences $\bx = \{x_0, x_1, \ldots\}$,
using a distribution over latent trees $\bz$.  
We are primarily interested in the posterior distribution over trees given a sentence, $p(\bz\mid\bx)$.
The performance of the generative model is secondary since we would like to analyze the
output of the unsupervised model for any linguistic regularities, and use the model as a 
testbed for linguistic hypotheses (of which I have none at the moment).

\citep{shen2018prpn}
\citet{yin18structvae}

\section{PRPN \citep{shen2018prpn}}
The Parsing-Reading-Predict Network \citep{shen2018prpn} is an approach to unsupervised tree induction
that uses a soft approximation to a latent variable to a degree of success.
The paper is inspired by the following hypothesis:
given a binary tree, a token at index $t$ only requires information up to index $l_t$ that satisfies
either of the following conditions:
\begin{itemize}
\item[(a)] the token at $l_t$ is the leftmost sibling of the token at $t$
\item[(b)] or, if the token at $t$ is a leftmost child, $l_t$ points to its parent's left sibling's leftmost child.
\end{itemize}
Although the hypothesis itself is not tested in the implementation and serves only as inspiration,
the model does see empirical success in the task of language modeling.
The model is realized through the following insight: given a ranking of tokens $\bx = \{x_0,\ldots,x_T\}$,
recursively splitting $\bx$ using the following procedure induces a binary tree where the first token to the left
of token $x_t$ that has higher rank, denoted $x_{l_t}$, also satisfies condition (a) or (b):
given token $i$ with the next highest rank, create a subtree $(x_{<i}, (x_i, x_{>i}))$ and
recursively perform this procedure until only terminal nodes remain.

\subsection{Ranking Models}
Before discussing the \citet{shen2018prpn} model, we first introduce
some notation and two possible ranking models, where both assign each item a latent score.
Let $\pi(i)$ denote the rank given to item $i$, and $\pi^{-1}(j)$ denote the 
item at rank $j$.
\begin{enumerate}
\item Plackett-Luce / Thurstone:
There is a hidden score $\theta_i$ assigned to every item $i$.
The $j$th ranking item is chosen from a categorical distribution 
parameterized by the output of a softmax over the remaining item's scores.
Here we have $\bz = \{\pi^{-1}(0),\pi^{-1}(1),\ldots,\pi^{-1}(T)\}$.
The probability of a given permutation is
$$
p(\bz)
= \prod_{i=0}^T\frac{\exp(\theta_{z_i})}{\sum_{k=i}^T \exp(\theta_{z_k})}
= \prod_{i=0}^T\frac{\exp(\theta_{\pi^{-1}(i)})}{\sum_{k=i}^T \exp(\theta_{\pi^{-1}(k)})}
$$
\item Pairwise Comparison Model (Bradley-Terry):
The same as PL, but samples pairwise comparisons from a bernoulli distribution
using the item's scores.
The probability that item $i$ is ranked higher than item $k$ is 
$\frac{\theta_{z_i}}{\theta_{z_i}+\theta_{z_k}}$.
Similary, we have the probability of a permutation is
$$
p(\bz) 
= \prod_{i=0}^T\prod_{k=i+1}^T\frac{\theta_{z_i}}{\theta_{z_i}+\theta_{z_k}}
$$
\end{enumerate}
One could use the Bradley-Terry (pairwise)
model of ranking for training and language modeling,
but then switch to the Plackett-Luce model at test time for outputting parse trees.
A soft pairwise model is used because it does not rely on future information and thus
is easy to train as a language model.
Once the scoring model is trained, we could use those scores to parameterize a global
PL/Thurstone model.

However, \citet{shen2018prpn} decide to instead
model as a RV the index $l_t$ (at every timestep) instead of the pairwise comparisons.
We will get into to the details of their choices in section~\ref{subsec:gates}.

\subsection{Their Model}
Let $x_t$ be the current token, $\bx_{0:t-1}$ all previous tokens,
$z_t$ the prediction for the current score,
and $\tilde{\bz}_{0:t-1}\in\R^{t-1}_+$ be all previous scores.
The model takes the form a language model,
where the distribution over the next token
$$p(x_t\mid \bx_{0:t-1}, \tilde{\bz}_{0:t-1},z_t) = f([h_{l_t:t-1},h_t])$$
is parameterized by either a linear projection or additional residual blocks
applied to a concatenation of the output of modified LSTMN (LSTM Memory-Network) $h_t$
and a convex combination of the previous LSTMN outputs (i.e. attention over $h_{l_t:t-1}$).
$f$ is referred to as the Predict Network.

\subsubsection{LSTMN}
\label{subsec:lstmn}
The LSTMN, referred to as the Reading Network, is as follows:
at each time step, the hidden and cell input are given by
$$\{\tilde{h}_t,\tilde{c}_t\} = \sum_{i=1}^{t-1} s_i^t\{h_i,c_i\},$$
a convex combination of the previous hidden and cell outputs.
Then $$h_t,c_t = \texttt{LSTM}(\tilde{h}_t,\tilde{c}_t).$$
The attention is computed in the same way as detailed in section~\ref{subsec:attn}.
The input to the LSTMN module is either the previous token or the
output of a previous LSTMN layer.

\subsubsection{Attention}
\label{subsec:attn}
As all attention modules in the PRPN network use the same formulation of attention we will use 
$y_t$ to refer to the input of the attention module,
$\bm{g}_t(l_t)$ the gate coefficients which are a function of the RV $l_t$,
$\bm{m}_t$ as the memories to attend over, and $\bm{s}_t$ as the attention coefficients.
The RV $l_t$ corresponds to the index of the token that satisfies condition (a) or (b).
First the key is computed using a linear projection of the input $k_t = W_ky_t$.
Then the intermediate scores are computed
$$\tilde{s}^i_t = \textrm{softmax}\left(\frac{m^i_tk^T_t}{\sqrt{\delta_k}}\right),$$
where $\delta_k$ is the dimension of the hidden state.
Finally, the gate coefficients are normalized and used to prevent the attention from
extending too far in the past.
$$s^i_t = \frac{g^i_t(l_t)}{\sum_jg^j_t(l_t)}\tilde{s}^i_t,$$
where the gate coefficients $\bm{g}_t(l_t)$ are detailed in section~\ref{subsec:gates}.

\subsubsection{Gating Self-attention}
\label{subsec:gates}
Recall that the gates $g^i_t$ also indirectly induce tree structure
(due to the previously stated insight that modulating self-attention
using a ranking induces a binary tree).
At every timestep for token $x_t$,
\citet{shen2018prpn} define a latent variable $l_t$ which corresponds to the index of the 
token satisfying either condition (a) or (b).

We then would have $$g^i_t(l_t) = \mathbf{1}(l_t \le i < t)$$
where we allow the model at timestep $t$ to only attend up to the token at leftmost limit $l_t$.
We would then renormalize the attention accordingly.
In order to calculate the gates $g^i_t(l_t)$ we must model $p(l_t\mid\bx_{0:t})$ 

(\citet{shen2018prpn} use the posterior distribution in their notation).
They propose to model 
$$p(l_t=i\mid\bx_{0:t-1})=(1-\alpha_i^t)\prod_{j=i+1}^{t-1}\alpha_j^t$$
using a stick-breaking process (GEM).
The reason this is preferred over a simple categorical distribution over indices is because
of the parameterization of the model.
Although unlikely, a bimodal distribution is possible with a categorical distribution.
I believe it would be possible to reparameterize the model and constrain the parameterization of a 
categorical distribution to achieve a similar effect.

Note that this itself is not a Dirichlet Process, as there is no base measure $H_0$.
In a $DP(\alpha, H_0)$ we have $\pi \sim GEM(\alpha)$,
where $b_i \sim Beta(1,\alpha)$ and $\pi_i = (1-b_i)\prod_{j=0}^{i-1}b_j$.
We then have $\theta_i\sim H_0$ and $G = \sum_i\pi_i\delta_{\theta_i}\sim DP(\alpha, H_0)$.
In the appendix of \citet{shen2018prpn} the say they use a Dirichlet process to model $p(l_t)$,
however this is not the case.
Another indication that they are not truly using a Dirichlet Process is the fact that
the only way $p(l_t) = DP$ is if the base distribution is Dirichlet as well.
This seems complicated and would definitely warrant further explanation.

Rather than resorting to approximate inference, they instead use the expected value of $g_i^t$
$$\E{g_i^t(l_t)} = F_{l_t}(l_t \le i\mid\bx_{0:t-1}) = \prod_{j=i+1}^{t-1}\alpha_j^t.$$
The $\alpha_j^t = \sigma(d_t - d_j)$, where $\sigma$ is the hard sigmoid function.
This is not stated in the paper (there are a couple off-by-one) errors,
but it must be that $g_{t-1}^t = 1$.
The $d_j\in\R_+$ are given by a CNN over the token embeddings.

The network used to compute the gates is called the Parsing Network.
When computing $p(x_t\mid\ldots)$, we cannot use the posterior score $d_t$,
so an estimate is used based on the previous $k$ words, where $k$ is the 
kernel width of the convolution.
However, afterwards \citet{shen2018prpn} use the posterior score $d_i \mid x_{i-K},\ldots, x_i$
for all $d_i$ when $i < t$.

\section{Training}
With the model that uses the expectation of $g_i^t(l_t)$
training is very simple.
We simply maximize the likelihood of the data directly since the model has no
sampling.

\section{Results}
\subsection{Language Modeling}

The language modeling results are decent, but it's unclear whether the
gating mechanism offers any improvement.
They perform ablation studies, but they do not seem to be replicable
so it is possible the perplexity improvements come 
mainly from embedding-tying and partly from self-attention.

\begin{table}[h!]                                                                 
\centering                                                                       
  \begin{tabular}{ c c }                                                         
    \toprule[2pt]                                                                
    Model & PPL \\                                                               
    \hline                                                                       
    RNN-LDA + KN-5 + cache \citep{mikolov2012context} &  92.0 \\                 
    LSTM \citep{zaremba2014recurrent} & 78.4 \\                                  
    Variational LSTM \citep{kim2016character} & 78.9 \\                          
    CharCNN \citep{kim2016character} & 78.9 \\                                   
    Pointer Sentinel-LSTM \citep{merity2016pointer} & 70.9 \\                    
    LSTM + continuous cache pointer \citep{grave2016improving} & 72.1 \\         
    Variational LSTM (tied) + augmented loss \citep{inan2016tying} & 68.5 \\     
    Variational RHN (tied) \citep{zilly2016recurrent} & 65.4 \\                  
    NAS Cell (tied)  \citep{zoph2016neural} & 62.4 \\                            
    4-layer skip connection LSTM (tied) \citep{melis2017state} & \textbf{58.3} \\
%     3-layer AWD-LSTM (tied) \citep{merity2017regularizing} & \textbf{57.3} \\  
    \hline                                                                       
    PRPN & 61.98 \\                                                              
    \toprule[2pt]                                                                
  \end{tabular}                                                                  
  \caption{PPL on the Penn Treebank test set}                                    
  \label{tab_ptb_word}                                                           
\end{table}                                                                      


\subsection{Parsing}
The parsing results, on the other hand, are quite good.
On WSJ10, the model beats the right-branching baseline but not the CCM model.
On the full WSJ, the model gets an averaged sentence F1 of 38.8,
which beats random, balanced, and right branching baselines
(the highest F1 reported is 21.3).
Why should we care about this?
There is no reason apriori that limiting self-attention should work at all
for learning to rank, as a fully left-branching structure would
allow the self-attention to function as normal. 

\begin{table}[h!]                                    
\centering                                          
  \begin{tabular}{ c c }                            
    \toprule[2pt]                                   
    Model & $\mathrm{UF}_1$ \\                      
    \hline                                          
    LBRANCH &  28.7 \\                              
    RANDOM & 34.7 \\                                
    DEP-PCFG \citep{carroll1992two} & 48.2 \\       
    RBRANCH & 61.7 \\                               
    CCM \citep{klein2002generative} & 71.9 \\       
    DMV+CCM \citep{klein2005natural} & 77.6 \\      
    UML-DOP \citep{bod2006all} & \textbf{82.9} \\   
    \hline                                          
    PRPN & 66.4 \\                                 
    \hline                                          
    UPPER BOUND & 88.1 \\                           
    \toprule[2pt]                                   
  \end{tabular}                                     
  \caption{Parsing Performance on the WSJ10 dataset}
  \label{tb_parser}                                 
\end{table}                                         


\begin{comment}
\section{Comparison to CCM \citep{klein-2002-ccm}}
\citep{klein-2002-ccm2}
\citep{golland-2012-ccm,huang-2012-ccm}
\section{Comparison to StructVae \citep{yin18structvae}}
Hm, if we generated context given span, would that encourage non-constituents?
General question about segmental models.

\section{Related Work}

\section{Alternative Latent Ranking Model}
\subsection{Generative Model}
PRPN \citep{shen2018prpn} choose to view $l_t$ as a latent variable.
There are at least a few other choices:
\begin{itemize}
\item Model the scores $d_t \sim \mathcal{N}(\mu_t,\sigma_t)$
or $d_t \sim \textrm{Gamma}$
and use the Plackett-Luce ranking distribution.
\item Model the comparisons $p(d_t < d_i)$ as order statistics of Gammas.
\item Model the permutation matrix $Z \sim \mathcal{B}_n$.
\item Model $l_t\sim \textrm{Cat}$.
\end{itemize}
Parameterize with $d_t\sim\mathcal{N}(\mu_t, \sigma_t)$.
Reparameterize comparisons with gumbel softmax?
Leave all self attentions as is?
\begin{itemize}
\item $p(z)$
\item $p(x|z)$
\end{itemize}

\section{Training and Inference}

for next time, convince Sasha of why this is interesting.
Also go over stick breaking process.
\end{comment}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

