\documentclass{article}

\usepackage[margin=1in]{geometry}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}

% bib
\usepackage[round]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Latent Constituents via Self-Attention}

\begin{document}
\maketitle

\section{Introduction}


\section{Problem}
We would like to learn a generative model
over source sentences $\bx = \{x_0, x_1, \ldots\}$,
using a distribution over latent trees $\bz$.  
\citep{shen2018prpn}
\citet{yin18structvae}

\section{PRPN \citep{shen2018prpn}}
The Parsing-Reading-Predict Network \citep{shen2018prpn} is an approach to unsupervised tree induction
that uses a soft approximation to a latent variable to a degree of success.
The paper is inspired by the following hypothesis:
given a binary tree, a token $x_t$ only requires information up to a token $x_{l_t}$ that satisfies
either of the following conditions:
\begin{itemize}
\item[(a)] its leftmost sibling
\item[(b)] or, if the token $x_t$ is a leftmost child, its parent's left sibling's leftmost child.
\end{itemize}
Although the hypothesis itself is not tested and seems to only serve as inspiration,
the model does see empirical success in the task of language modeling.
The model is realized through the following insight: given a ranking of tokens $\bx = \{x_0,\ldots,x_T\}$,
recursively splitting $\bx$ using the following procedure induces a binary tree where the first token to the left
of token $x_t$ that has higher rank, denoted $x_{l_t}$, also satisfies condition (a) or (b):
given token $i$ with the next highest rank, create a subtree $(x_{<i}, (x_i, x_{>i}))$ and
recursively perform this procedure until only terminal nodes remain.

[Example on board]

\subsection{The Model}
Let $x_t$ be the current token, $\bx_{0:t-1}$ all previous tokens, $z_t$ the prediction for the current score,
and $\tilde{\bz}_{0:t-1}\in\R^{t-1}_+$ be all previous scores.
The model takes the form a language model, where
the distribution over the next token
$$p(x_t\mid \bx_{0:t-1}, \tilde{\bz}_{0:t-1},z_t) = f([h_{l_t:t-1},h_t])$$
is parameterized by either a linear projection or additional residual blocks
applied to a concatenation of the output of modified LSTMN (LSTM Memory-Network) $h_t$
and a convex combination of the previous LSTMN outputs (attention over $h_{l_t:t-1}$).
$f$ is referred to as the Predict Network, and we will cover the attention mechanism shortly.
The LSTMN, referred to as the Reading Network, is as follows:
at each time step, the hidden and cell input are given by
$$\{\tilde{h}_t,\tilde{c}_t\} = \sum_{i=1}^{t-1} s_i^t\{h_i,c_i\},$$
a convex combination of the previous hidden and cell outputs.
Then $$h_t,c_t = \texttt{LSTM}(\tilde{h}_t,\tilde{c}_t).$$
The coefficients $s_i^t$ are computed by first predicting a key $$k = W_hh_{t-1}+W_xx_t$$
then querying all previous hidden states in order to compute the intermediate attention
$$\tilde{s}_i^t = \textrm{softmax}(\frac{h_ik_t^T}{\sqrt{\delta_k}}),$$
where $\delta_k$ is the dimension of the hidden state.
This same attention mechanism is used in the Predict Network $f$.
We then incorporate a form of multiplicative gating $g_i^t$
that is a function of the induced ranking of the tokens,
which we described in the previous section.
Finally,
$$s_i^t = \frac{g_i^t}{\sum_jg_j^t}\tilde{s_i^t},$$
so at every timestep $t$ the self-attention is renormalized using the gates $g_{l_t:t-1}^t$.

\subsection{Gating Self-attention}
Recall that the gates $g_i^t$ used modulate self-attention also indirectly induce tree structure
(due to the previously stated insight that modulating self-attention
using a ranking induces a binary tree).
At every timestep for token $x_t$,
\citet{shen2018prpn} define a latent variable $l_t$ which corresponds to the index of the 
token satisfying either condition (a) or (b).
We then would have $$g_i^t = \left\{\begin{array}{lr}
1, & l_t \le i < t\\
0, & \textrm{otherwise.}
\end{array}\right.$$
where we allow the model at timestep $t$ to only attend up to the token at $l_t$.
We would then renormalize the attention accordingly.
In order to calculate the gates $g_i^t$ we must model $p(l_t\mid\bx_{0:t})$ 
(\citet{shen2018prpn} use the posterior distribution in their notation).
They propose to model 
$$p(l_t=i\mid\bx_{0:t-1})=(1-\alpha_i^t)\prod_{j=i+1}^{t-1}\alpha_j^t$$
using a stick-breaking process.
Rather than resorting to approximate inference, they instead use the expected value of $g_i^t$
$$\E{g_i^t} = F_{l_t}(l_t \le i\mid\bx_{0:t-1}) = \prod_{j=i+1}^{t-1}\alpha_j^t.$$
\citet{shen2018prpn} derive the expectation in the appendix of their paper.
The $\alpha_j^t = \sigma(d_t - d_j)$, where $\sigma$ is the hard sigmoid function.
The $d_j\in\R_+$ are given by a CNN over the token embeddings.

The network used to compute the gates is called the Parsing Network.
When computing $p(x_t\mid\ldots)$, we cannot use the posterior score $d_t$,
so an estimate is used based on the previous $k$ words, where $k$ is the 
kernel width of the convolution.

\section{Comparison to CCM \citep{klein-2002-ccm}}
\citep{klein-2002-ccm2}
\citep{golland-2012-ccm,huang-2012-ccm}

\section{Alternative Latent Ranking Model}
\subsection{Generative Model}
PRPN \citep{shen2018prpn} choose to view $l_t$ as a latent variable.
There are at least a few other choices:
\begin{itemize}
\item Model the scores $d_t \sim \mathcal{N}(\mu_t,\sigma_t)$
or $d_t \sim \textrm{Gamma}$
and use the Plackett-Luce ranking distribution.
\item Model the comparisons $p(d_t < d_i)$ as order statistics of Gammas.
\item Model the permutation matrix $Z \sim \mathcal{B}_n$.
\item Model $l_t\sim \textrm{Cat}$.
\end{itemize}
Parameterize with $d_t\sim\mathcal{N}(\mu_t, \sigma_t)$.
Reparameterize comparisons with gumbel softmax?
Leave all self attentions as is?
\begin{itemize}
\item $p(z)$
\item $p(x|z)$
\end{itemize}

\section{Training and Inference}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

