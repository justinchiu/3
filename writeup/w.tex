\documentclass{article}

\usepackage[margin=1in]{geometry}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}

% bib
\usepackage[round]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{booktabs}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Latent Constituents via Self-Attention}

\begin{document}
\maketitle

\section{Introduction}


\section{Problem}
We would like to learn a generative model
over source sentences $\bx = \{x_0, x_1, \ldots\}$,
using a distribution over latent trees $\bz$.  
We are primarily interested in the posterior distribution over trees given a sentence, $p(\bz\mid\bx)$.
The performance of the generative model is secondary since we would like to analyze the
output of the unsupervised model for any linguistic regularities, and use the model as a 
testbed for linguistic hypotheses (of which I have none at the moment).

\citep{shen2018prpn}
\citet{yin18structvae}

\section{PRPN \citep{shen2018prpn}}
The Parsing-Reading-Predict Network \citep{shen2018prpn} is an approach to unsupervised tree induction
that uses a soft approximation to a latent variable to a degree of success.

\section{The Language Model}
Let $\bx$ be all tokens
and $\bl$ all latent variables with each $l_t\in\set{0,\ldots,t-1}$ a categorical RV over all previous indices.
In addition to modeling the tokens,
\citet{shen2018prpn} introduce a latent variable at every timestep $l_t$. 
They decompose the joint distribution 
\begin{equation}
p(\bx, \bl)
= \prod_t p(x_{t+1},l_t\mid\bx_{\le t},\bl_{<t})
= \prod_t p(x_{t+1}\mid l_t,\bx_{\le t},\bl_{<t})p(l_t\mid\bx_{\le t}).
\end{equation}

\subsection{$p(x_{t+1}\mid l_t,\bx_{\le t},\bl_{<t})$}
The first term takes the form of a self-attentive language model,
where the distribution over the next token
$$p(x_{t+1}\mid l_t,\bx_{\le t},\bl_{<t}) = \textrm{softmax}(f(\bm{m}_{<t}, l_t))$$
where $f$ is either a linear projection or additional residual blocks,
composed with the output of an LSTMN (LSTM Memory-Network),
as well as the latent variable $l_t$.
The LSTMN attends over the previous $\bm{m}_{<t}$ to produce $m_t$,
the current hidden state.
This is referred to as the Predict Network.

The attention mechanism is defined similarly throughout their network.
The recurrent input to the current timestep is a convex combination of
all previous outputs of the current layer.
Let $\bm{m}_{<t}$ be the concatenated output of all previous timesteps,
then the recurrent input would be
$$c = \sum_{i=0}^{t-1} s_i(l_t) m_i$$
and the output of the layer at the current timestep would be
$m_t = LSTM(c, x_t)$ where $s_i$ is the attention coefficient
for that index at timestep $t$ and $x_t$ is the input.

The attention coefficients are a renormalized weighted sum of
the dot-product attention we are used to.
Let $\tilde{\bm{s}_t}\in\Delta^{t-2}$, the $t-2$ simplex, be the output of a softmax over
the dot-product of memories $\bm{m}_{<t}$ and query $k_t$
which is a linear function of the input $x_t$.
We then define
$$s_i(l_t) = \frac{g_i(l_t)\tilde{s}_i}{\sum_jg_j(l_t)\tilde{s}_j},$$
where $g_i(l_t) = \mathbf{1}(l_t \le i)$.
We have a set of these \textbf{per timestep}.
Note that although we define $p(l_t)$ in the following section,
we will end up using $\Es{l_t}{g_i(l_t)}$ as a soft approximation to the
latent variable model.

[Work through figure 1]

\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 0 & 1 & 2 & 3 & 4\\
\hline
$\tilde{s}$ & 0.4 & 0.1 & 0.3 & 0.2 & \\
\hline
$g\mid l_4=2$         & 0   & 1   & 1   & 1   & - \\
\hline
$s(l_4=2)$     & 0 & 0.17 & 0.5 & 0.33 & -\\
\hline
\end{tabular}
\caption{An example computation of the attention gates.
Let $\alpha_i = \frac{d_t-d_i+1}{2}$ and $t=4$.}
\end{figure}
\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 0 & 1 & 2 & 3 & 4\\
\hline
$\tilde{s}$ & 0.4 & 0.1 & 0.3 & 0.2 & \\
\hline
$d$         & 0.7 & 0.6 & 0.3 & 0.4 & 0.5\\
\hline
$\alpha$    & - & 0.45 & 0.6 & 0.55 & -\\
\hline
$p(l_4)$    & 0.45*0.6*0.55=0.1485 & (1-0.45)*0.6*0.55=0.1815 & (1-0.6)*0.55=0.22 & 1-0.55=0.45 &\\
\hline
$F_{l_4}$   & 0.1485 & 0.1815+0.1485=0.33=0.6*0.55 & 0.22+0.1815+0.1485=0.55 & 1 &\\
\hline
$s(\E{l_4})$ & 0.13 & 0.07 & 0.36 & 0.43 & -\\
\hline
\end{tabular}
\caption{An example computation of the attention gates.
Let $\alpha_i = \frac{d_t-d_i+1}{2}$ and $t=4$.}
\end{figure}

\subsection{$p(l_t\mid\bx_{\le t})$}
We define
\begin{equation*}
p(l_t=k\mid\bx_{\le t}) =\left\{\begin{array}{l l}
1-\alpha_{t-1}, & k=t-1\\
\prod_{j=1}^{t-1}\alpha_j, & k=0\\
(1-\alpha_k)\prod_{j=k+1}^{t-1}\alpha_j, & \textrm{otherwise}
\end{array}\right.
\end{equation*}
Where the $\alpha_k$ are drawn from `a Beta distribution'.
This appears to be inspired by the stick-breaking process (GEM).
But this cannot be called a Dirichlet Process, as a $DP(\alpha, H_0)$ is defined by
$b_i\sim\Betad(1,\alpha),\pi_i=(1-b_i)\prod_{j=0}^{i-1}b_i,\bm{\pi}\sim GEM(\alpha)$
and with $\theta_i\sim H_0$ we have $G = \sum_i\pi_i\delta_{\theta_i}\sim DP(\alpha, H_0)$.
This would only make sense in this context if the base distribution $H_0$ were also Dirichlet,
but this would likely warrant further discussion.

Rather than proceed further in defining the generative model,
\citet{shen2018prpn} instead claim that instead of using $g_i(l_t)$
they use its expectation (taken wrt $l_t$).
We have
$$
\Es{l_t}{g_i(l_t)} = \prod_{j=i+1}^{t-1}\alpha_j,
$$
and we then use
$$\Es{l_t}{s_i(l_t)} = \frac{\Es{l_t}{g_i(l_t)}\tilde{s}_i}{\sum_j\Es{l_t}{g_j(l_t)}\tilde{s}_j}$$
as the attention coefficients instead of $s_i(l_t)$.
They then parameterize the $\alpha_k$ of timestep $t$ deterministically as follows:
$$\alpha_k = \frac{\textrm{hardtanh}(2(d_t-d_k)/\tau+1)+1}{2},$$
where $\tau$ is some scaling parameter.
The $d_k$ are obtained from a CNN over the tokens $\bx_{k-L:k}$,
where $L$ is the width of the convolution kernel (this is set to 5).

[Work through Figure 2]

\section{Training}
With the model that uses the expectation of $g_i^t(l_t)$ training is very simple.
We simply maximize the likelihood of the data directly since the model 
is fully soft.

\section{Results}
\subsection{Language Modeling}

The language modeling results are decent, but it's unclear whether the
gating mechanism offers any improvement.
They perform ablation studies, but they do not seem to be replicable
so it is possible the perplexity improvements come 
mainly from embedding-tying and partly from self-attention.

\begin{table}[h!]                                                                 
\centering                                                                       
  \begin{tabular}{ c c }                                                         
    \toprule[2pt]                                                                
    Model & PPL \\                                                               
    \hline                                                                       
    RNN-LDA + KN-5 + cache \citep{mikolov2012context} &  92.0 \\                 
    LSTM \citep{zaremba2014recurrent} & 78.4 \\                                  
    Variational LSTM \citep{kim2016character} & 78.9 \\                          
    CharCNN \citep{kim2016character} & 78.9 \\                                   
    Pointer Sentinel-LSTM \citep{merity2016pointer} & 70.9 \\                    
    LSTM + continuous cache pointer \citep{grave2016improving} & 72.1 \\         
    Variational LSTM (tied) + augmented loss \citep{inan2016tying} & 68.5 \\     
    Variational RHN (tied) \citep{zilly2016recurrent} & 65.4 \\                  
    NAS Cell (tied)  \citep{zoph2016neural} & 62.4 \\                            
    4-layer skip connection LSTM (tied) \citep{melis2017state} & \textbf{58.3} \\
%     3-layer AWD-LSTM (tied) \citep{merity2017regularizing} & \textbf{57.3} \\  
    \hline                                                                       
    PRPN & 61.98 \\                                                              
    \toprule[2pt]                                                                
  \end{tabular}                                                                  
  \caption{PPL on the Penn Treebank test set}                                    
  \label{tab_ptb_word}                                                           
\end{table}                                                                      


\subsection{Parsing}
The parsing results, on the other hand, are quite good.
On WSJ10, the model beats the right-branching baseline but not the CCM model.
On the full WSJ, the model gets an averaged sentence F1 of 38.8,
which beats random, balanced, and right branching baselines
(the highest F1 reported is 21.3).
Why should we care about this?
There is no reason apriori that limiting self-attention should work at all
for learning to rank, as a fully left-branching structure would
allow the self-attention to function as normal. 

\begin{table}[h!]                                    
\centering
  \begin{tabular}{ c c }
    \toprule[2pt]
    Model & $\mathrm{UF}_1$ \\
    \hline
    LBRANCH &  28.7 \\
    RANDOM & 34.7 \\
    DEP-PCFG \citep{carroll1992two} & 48.2 \\
    RBRANCH & 61.7 \\
    CCM \citep{klein2002generative} & 71.9 \\
    DMV+CCM \citep{klein2005natural} & 77.6 \\
    UML-DOP \citep{bod2006all} & \textbf{82.9} \\
    \hline
    PRPN & 66.4 \\
    \hline                                          
    UPPER BOUND & 88.1 \\                           
    \toprule[2pt]                                   
  \end{tabular}                                     
  \caption{Parsing Performance on the WSJ10 dataset}
  \label{tb_parser}                                 
\end{table}                                         

\subsection{Ranking Models}
Before discussing the \citet{shen2018prpn} model, we first introduce
some notation and two possible ranking models, where both assign each item a latent score.
Let $\pi(i)$ denote the rank given to item $i$, and $\pi^{-1}(j)$ denote the 
item at rank $j$.
\begin{enumerate}
\item Plackett-Luce / Thurstone:
There is a hidden score $\theta_i$ assigned to every item $i$.
The $j$th ranking item is chosen from a categorical distribution 
parameterized by the output of a softmax over the remaining items' scores.
Here we have $\bz = \{\pi^{-1}(0),\pi^{-1}(1),\ldots,\pi^{-1}(T)\}$.
The probability of a given permutation is
$$
p(\bz)
= \prod_{i=0}^T\frac{\exp(\theta_{z_i})}{\sum_{k=i}^T \exp(\theta_{z_k})}
= \prod_{i=0}^T\frac{\exp(\theta_{\pi^{-1}(i)})}{\sum_{k=i}^T \exp(\theta_{\pi^{-1}(k)})}
$$
\item Pairwise Comparison Model (Bradley-Terry):
The same as PL, but samples pairwise comparisons from a bernoulli distribution
using the item's scores.
The probability that item $i$ is ranked higher than item $k$ is 
$\frac{\theta_{z_i}}{\theta_{z_i}+\theta_{z_k}}$.
Similary, we have the probability of a permutation is
$$
p(\bz) 
= \prod_{i=0}^T\prod_{k=i+1}^T\frac{\theta_{z_i}}{\theta_{z_i}+\theta_{z_k}}
$$
\end{enumerate}
One could use the Bradley-Terry (pairwise)
model of ranking for training a language model,
but then switch to the Plackett-Luce model at test time for outputting parse trees.
This would allow us to learn a scoring model which can then be shared between the 
pairwise and global ranking models.
In the pairwise model, we could model
$$l_t = \max \set{i : \alpha_t^i = 1, 0\leq i<t},
\alpha_t^i \sim \Bern\left(\frac{z_t}{z_t+z_i}\right).$$

However, \citet{shen2018prpn} decide to instead
model as a RV the index $l_t$ (at every timestep) instead of the pairwise comparisons.
We will get into to the details of their choices in section~\ref{subsec:gates}.

The paper is inspired by the following hypothesis:
given a binary tree, a token at index $t$ only requires information up to index $l_t$ that satisfies
either of the following conditions:
\begin{itemize}
\item[(a)] the token at $l_t$ is the leftmost sibling of the token at $t$
\item[(b)] or, if the token at $t$ is a leftmost child, $l_t$ points to its parent's left sibling's leftmost child.
\end{itemize}
Although the hypothesis itself is not tested in the implementation and serves only as inspiration,
the model does see empirical success in the task of language modeling.
The model is realized through the following insight: given a ranking of tokens $\bx = \{x_0,\ldots,x_T\}$,
recursively splitting $\bx$ using the following procedure induces a binary tree where the first token to the left
of token $x_t$ that has higher rank, denoted $x_{l_t}$, also satisfies condition (a) or (b):
given token $i$ with the next highest rank, create a subtree $(x_{<i}, (x_i, x_{>i}))$ and
recursively perform this procedure until only terminal nodes remain.

\begin{comment}
\section{Comparison to CCM \citep{klein-2002-ccm}}
\citep{klein-2002-ccm2}
\citep{golland-2012-ccm,huang-2012-ccm}
\section{Comparison to StructVae \citep{yin18structvae}}
Hm, if we generated context given span, would that encourage non-constituents?
General question about segmental models.

\section{Related Work}

\section{Alternative Latent Ranking Model}
\subsection{Generative Model}
PRPN \citep{shen2018prpn} choose to view $l_t$ as a latent variable.
There are at least a few other choices:
\begin{itemize}
\item Model the scores $d_t \sim \mathcal{N}(\mu_t,\sigma_t)$
or $d_t \sim \textrm{Gamma}$
and use the Plackett-Luce ranking distribution.
\item Model the comparisons $p(d_t < d_i)$ as order statistics of Gammas.
\item Model the permutation matrix $Z \sim \mathcal{B}_n$.
\item Model $l_t\sim \textrm{Cat}$.
\end{itemize}
Parameterize with $d_t\sim\mathcal{N}(\mu_t, \sigma_t)$.
Reparameterize comparisons with gumbel softmax?
Leave all self attentions as is?
\begin{itemize}
\item $p(z)$
\item $p(x|z)$
\end{itemize}

\section{Training and Inference}

for next time, convince Sasha of why this is interesting.
Also go over stick breaking process.
\end{comment}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

