\documentclass{article}

\usepackage[margin=1in]{geometry}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage{titlesec}

% bib
\usepackage[round]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Latent Constituents via Self-Attention}

\begin{document}
\maketitle

\section{Introduction}


\section{Problem}
We would like to learn a generative model
over source sentences $\bx = \{x_0, x_1, \ldots\}$,
using a distribution over latent trees $\bz$.  
\citep{shen2018prpn}
\citet{yin18structvae}

\section{PRPN \citep{shen2018prpn}}
The Parsing-Reading-Predict Network \citep{shen2018prpn} is an approach to unsupervised tree induction
that uses a soft approximation to a latent variable to a degree of success.
The paper is inspired by the following hypothesis:
given a binary tree, a token $x_t$ only requires information up to a token $x_{l_t}$ that satisfies
either of the following conditions:
\begin{itemize}
\item[(a)] its leftmost sibling
\item[(b)] or, if the token $x_t$ is a leftmost child, its parent's left sibling's leftmost child.
\end{itemize}
Although the hypothesis itself is not tested and seems to only serve as inspiration,
the model does see empirical success in the task of language modeling.
The model is realized through the following insight: given a ranking of tokens $\bx = \{x_0,\ldots,x_T\}$,
recursively splitting $\bx$ using the following procedure induces a binary tree where the first token to the left
of token $x_t$ that has higher rank, denoted $x_{l_t}$, also satisfies condition (a) or (b):
given token $i$ with the next highest rank, create a subtree $(x_{<i}, (x_i, x_{>i}))$ and
recursively perform this procedure until only terminal nodes remain.

\subsection{The Model}
Let $x_t$ be the current token, $\bx_{0:t-1}$ all previous tokens, $z_t$ the prediction for the current score,
and $\tilde{\bz}_{0:t-1}\in\R^{t-1}_+$ be all previous scores.
The model takes the form a language model, where
the distribution over the next token
$$p(x_t\mid \bx_{0:t-1}, \tilde{\bz}_{0:t-1},z_t) = f(h_t)$$
is parameterized by either a linear projection or additional residual blocks
applied to the output of modified LSTMN (LSTM Memory-Network) $h_t$.
$f$ is referred to as the Predict Network.
The LSTMN, referred to as the Reading Network, is as follows:
at each time step, the hidden and cell input are given by
$$\{\tilde{h}_t,\tilde{c}_t\} = \sum_{i=1}^{t-1} s_i^t\{h_i,c_i\},$$
a convex combination of the previous hidden and cell outputs.
Then $$h_t,c_t = \texttt{LSTM}(\tilde{h}_t,\tilde{c}_t).$$
The coefficients $s_i^t$ are computed by first predicting a key $$k = W_hh_{t-1}+W_xx_t$$
and querying all previous hidden states in order to compute the intermediate 
$$\tilde{s}_i^t = \textrm{softmax}(\frac{h_ik_t^T}{\sqrt{\delta_k}}),$$
where $\delta_k$ is the dimension of the hidden state.
We then incorporate a form of multiplicative gating $g_i^t$ that is a function of the induced ranking of the tokens,
which we described in the previous section.
Finally,
$$s_i^t = \frac{g_i^t}{\sum_jg_j^t}\tilde{s_i^t},$$
so at every timestep $t$ the self-attention is renormalized using the gates $g_{l_t:t-1}^t$.

\subsection{Gating Self-attention}
Recall that the gates $g_i^t$ are used to both induce a latent tree and modulate self-attention
(due to the previously stated insight that modulating self-attention using a ranking induces a binary tree).
At every timestep for token $x_t$, we define a latent variable $l_t$ which corresponds to the index of the 
token satisfying either condition (a) or (b).
We then would have $$g_i^t = \left\{\begin{array}{lr}
1, & l_t \le i < t\\
0, & \textrm{otherwise}
\end{array}\right.$$



\section{Model}
\subsection{Generative Model}
Parameterize with $d_t\sim\mathcal{N}(\mu_t, \sigma_t)$.
Reparameterize comparisons with gumbel softmax?
Leave all self attentions as is?
\begin{itemize}
\item $p(z)$
\item $p(x|z)$
\end{itemize}

\section{Training and Inference}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

